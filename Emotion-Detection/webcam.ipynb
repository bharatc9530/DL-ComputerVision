{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webcam prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#live video\n",
    "\n",
    "classifier = cv2.CascadeClassifier(f'haarcascade_frontalface_default.xml')\n",
    "\n",
    "emotion_model = load_model('bestweight.h5')\n",
    "\n",
    "\n",
    "emotion_dim = (48,48)\n",
    "\n",
    "emotion_bw = False \n",
    "\n",
    "emotion_dict = {0: 'Angry', 1: 'Happy', 2: 'Neutral', 3: 'sad', 4: 'disgust', 5: 'fear', 6: 'surprise'}\n",
    "\n",
    "vid_frames = []\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap_video = False\n",
    "\n",
    "if cap_video == True:\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width  = cap.get(cv2.CAP_PROP_FRAME_WIDTH)   # float\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('EmotionDetector.mp4', fourcc,10, (int(width), int(height)))\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1,1)\n",
    "    clone = frame.copy()\n",
    "    bboxes = classifier.detectMultiScale(clone)\n",
    "    for i in bboxes: \n",
    "        x, y, width, height = i[0], i[1], i[2], i[3]\n",
    "        x2, y2 = x+ width, y+height\n",
    "        emotion_roi = clone[y:y2, x:x2]\n",
    "        emotion_roi= cv2.cvtColor(emotion_roi, cv2.COLOR_BGR2GRAY)\n",
    "        emotion_roi = cv2.resize(emotion_roi, emotion_dim, interpolation = cv2.INTER_CUBIC)\n",
    "        emotion_roi = emotion_roi/255\n",
    "        emotion_roi = emotion_roi.reshape(1, emotion_roi.shape[0], emotion_roi.shape[1], 1)\n",
    "        #emotion predictions\n",
    "        emotion_predict = emotion_model.predict(emotion_roi)[0]\n",
    "        emotion_idx = np.argmax(emotion_predict)\n",
    "        emotion_cat = emotion_dict[emotion_idx]\n",
    "        emotion_conf = f'{round(np.max(emotion_predict)*100)}%'\n",
    "        cv2.putText(clone, f'{emotion_cat}: {emotion_conf}', (x, y+15), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,225), 2)\n",
    "        cv2.rectangle(clone, (x,y), (x2,y2), (0,225,0), 1)\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "    cv2.imshow('LIVE', clone)\n",
    "    vid_frames.append(clone)\n",
    "    if cap_video == True:\n",
    "        out.write(clone)\n",
    "    if cv2.waitKey(1) & 0xFF ==ord('q'): \n",
    "        break \n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if cap_video == True:\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile Camera prediction provide IP in url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#live video\n",
    "\n",
    "classifier = cv2.CascadeClassifier(f'haarcascade_frontalface_default.xml')\n",
    "\n",
    "emotion_model = load_model('bestweight.h5')\n",
    "\n",
    "\n",
    "emotion_dim = (48,48)\n",
    "\n",
    "emotion_bw = False \n",
    "\n",
    "emotion_dict = {0: 'Angry', 1: 'Happy', 2: 'Neutral', 3: 'sad', 4: 'disgust', 5: 'fear', 6: 'surprise'}\n",
    "\n",
    "vid_frames = []\n",
    "#url = 'http://192.168.43.105:8080/video'\n",
    "#cap = cv2.VideoCapture(url)\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap_video = False\n",
    "\n",
    "if cap_video == True:\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width  = cap.get(cv2.CAP_PROP_FRAME_WIDTH)   # float\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('Tests/FaceDetector.mp4', fourcc,10, (int(width), int(height)))\n",
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1,1)\n",
    "    clone = frame.copy()\n",
    "    bboxes = classifier.detectMultiScale(clone)\n",
    "    for i in bboxes: \n",
    "        x, y, width, height = i[0], i[1], i[2], i[3]\n",
    "        x2, y2 = x+ width, y+height\n",
    "        emotion_roi = clone[y:y2, x:x2]\n",
    "        emotion_roi= cv2.cvtColor(emotion_roi, cv2.COLOR_BGR2GRAY)\n",
    "        emotion_roi = cv2.resize(emotion_roi, emotion_dim, interpolation = cv2.INTER_CUBIC)\n",
    "        emotion_roi = emotion_roi/255\n",
    "        emotion_roi = emotion_roi.reshape(1, emotion_roi.shape[0], emotion_roi.shape[1], 1)\n",
    "        #emotion predictions\n",
    "        emotion_predict = emotion_model.predict(emotion_roi)[0]\n",
    "        emotion_cat1 = emotion_dict[0]\n",
    "        emotion_cat2 = emotion_dict[1]\n",
    "        emotion_cat3 = emotion_dict[2]\n",
    "        emotion_conf1 = f'{round(emotion_predict[0]*100)}%'\n",
    "        emotion_conf2 = f'{round(emotion_predict[1]*100)}%'\n",
    "        emotion_conf3 = f'{round(emotion_predict[2]*100)}%'\n",
    "        cv2.putText(clone, f'{emotion_cat3}: {emotion_conf3}', (x+5, y+50), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,225), 2)\n",
    "        cv2.putText(clone, f'{emotion_cat1}: {emotion_conf1}', (x+5, y+30), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,225), 2)\n",
    "        cv2.putText(clone, f'{emotion_cat2}: {emotion_conf2}', (x+5, y+15), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,225), 2)\n",
    "        cv2.rectangle(clone, (x,y), (x2,y2), (0,225,0), 2)\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "    cv2.imshow('LIVE', clone)\n",
    "    vid_frames.append(clone)\n",
    "    if cap_video == True:\n",
    "        out.write(clone)\n",
    "    if cv2.waitKey(1) & 0xFF ==ord('q'): \n",
    "        break \n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if cap_video == True:\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(vid_frames): \n",
    "    cv2.imwrite(f'Tests/VidFrames/{idx}.png', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected Ptr<cv::UMat> for argument 'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9f034c90d829>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstack_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Images/DemoStack.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Expected Ptr<cv::UMat> for argument 'img'"
     ]
    }
   ],
   "source": [
    "img_loc = [22,52,74, 103]\n",
    "\n",
    "images = []\n",
    "for i in img_loc:\n",
    "    img = cv2.imread(f'Tests/VidFrames/{i}.png')\n",
    "    images.append(img)\n",
    "    \n",
    "\n",
    "stack_img = np.hstack(images)\n",
    "cv2.imwrite('Images/DemoStack.png', stack_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
